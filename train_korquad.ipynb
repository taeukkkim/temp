{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from solution.args import HfArgumentParser\n",
    "from solution.args import (\n",
    "    MrcDataArguments,\n",
    "    MrcModelArguments,\n",
    "    MrcTrainingArguments,\n",
    "    MrcProjectArguments,\n",
    ")\n",
    "\n",
    "parser = HfArgumentParser(\n",
    "    [MrcDataArguments,\n",
    "     MrcModelArguments,\n",
    "     MrcTrainingArguments,\n",
    "     MrcProjectArguments]\n",
    ")\n",
    "args = parser.parse_yaml_file(yaml_file=\"configs/example.yaml\")\n",
    "data_args, model_args, training_args, project_args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MrcModelArguments(model_name_or_path='monologg/koelectra-small-v3-discriminator', reader_type='extractive', architectures='AutoModelForQuestionAnswering', config_name=None, tokenizer_name=None, model_cache_dir='None', model_init='basic', use_auth_token='Fasle', revision='main', model_head='None', qa_conv_out_channel=1024, qa_conv_input_size=384, qa_conv_n_layers=5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./outputs/koelectra_v3_test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "noanswer_data = load_from_disk(\"./data/aihub/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.pad_to_max_length = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.data.processors.prep import PREP_PIPELINE\n",
    "\n",
    "prep_fn, is_batched = PREP_PIPELINE[\"extractive\"](\n",
    "    tokenizer, \"train\", data_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_COLUMN_NAME = \"question\"\n",
    "CONTEXT_COLUMN_NAME = \"context\"\n",
    "ANSWER_COLUMN_NAME = \"answers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    \"\"\"Tokenize questions and contexts\n",
    "\n",
    "    Args:\n",
    "        examples (Dict): DatasetDict\n",
    "\n",
    "    Returns:\n",
    "        Dict: Tokenized examples\n",
    "    \"\"\"\n",
    "\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    max_seq_length = min(data_args.max_seq_length,\n",
    "                         tokenizer.model_max_length)\n",
    "\n",
    "    # truncation과 padding을 통해 tokenization을 진행\n",
    "    # stride를 이용하여 overflow를 유지\n",
    "    # 각 example들은 이전의 context와 조금씩 겹침\n",
    "    # overflow 발생 시 지정한 batch size보다 더 많은 sample이 들어올 수 있음 -> data augmentation\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[QUESTION_COLUMN_NAME if pad_on_right else CONTEXT_COLUMN_NAME],\n",
    "        examples[CONTEXT_COLUMN_NAME if pad_on_right else QUESTION_COLUMN_NAME],\n",
    "        # 길이가 긴 context가 등장할 경우 truncation을 진행\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=data_args.doc_stride,\n",
    "        # overflow 발생 시 원래 인덱스를 찾을 수 있게 mapping 가능한 값이 필요\n",
    "        return_overflowing_tokens=True,\n",
    "        # token의 캐릭터 단위 position을 찾을 수 있는 offset을 반환\n",
    "        # start position과 end position을 찾는데 도움을 줌\n",
    "        return_offsets_mapping=True,\n",
    "        # sentence pair가 입력으로 들어올 때 0과 1로 구분지음\n",
    "        return_token_type_ids=data_args.return_token_type_ids,\n",
    "        padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        # return_tensors='pt'\n",
    "    )\n",
    "    return tokenized_examples\n",
    "    \n",
    "def prepare_train_features(examples):\n",
    "    \"\"\"\n",
    "    Reset for train dataset that do not have the correct answer \n",
    "    or where the correct answer position has changed.\n",
    "\n",
    "    Args:\n",
    "        examples (Dict): DatasetDict\n",
    "\n",
    "    Returns:\n",
    "        Dict: Tokenized examples where the answer has been reset\n",
    "    \"\"\"\n",
    "\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    tokenized_examples = tokenize_fn(examples)\n",
    "    \n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 데이터셋에 \"start position\", \"enc position\" label을 부여합니다.\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index\n",
    "\n",
    "        # sequence id를 설정합니다 (context와 question을 구분).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 길이가 긴 context에 대해 truncation을 진행하기 때문에\n",
    "        # 하나의 example이 여러 개의 span을 가질 수 있음\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[ANSWER_COLUMN_NAME][sample_index]\n",
    "\n",
    "        # answer가 없을 경우 cls_index를 answer로 설정\n",
    "        # example에서 정답이 없는 경우가 있을 수 있음\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # text에서 정답의 start/end character index를 가져옴\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # sequence_ids는 0, 1, None의 세 값만 가짐\n",
    "            # None 0 0 ... 0 None 1 1 ... 1 None\n",
    "\n",
    "            # text에서 context가 시작하는 위치로 이동\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != context_index:\n",
    "                token_start_index += 1\n",
    "\n",
    "            # text에서 context가 끝나는 위치로 이동\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != context_index:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 정답이 span을 벗어나는지 체크.\n",
    "            # 정답이 없는 경우 CLS index로 labeling (Retro일 경우 다르게 처리)\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char and\n",
    "                offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # token_start_index 및 token_end_index를 answer의 끝으로 이동\n",
    "                # Note: answer가 마지막 단어인 경우 last offset을 따라갈 수 있음\n",
    "\n",
    "                # token_start_index를 실제 위치로 맞춰주는 과정\n",
    "                while (\n",
    "                    token_start_index < len(offsets) and\n",
    "                    offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                tokenized_examples[\"start_positions\"].append(\n",
    "                    token_start_index - 1)\n",
    "\n",
    "                # token_end_index를 실제 위치로 맞춰주는 과정\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                tokenized_examples[\"end_positions\"].append(\n",
    "                    token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b444fa8e3dd04c46987c4c7c8a1faaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = noanswer_data[\"train\"].map(\n",
    "    prep_fn,\n",
    "    batched=is_batched,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    remove_columns=noanswer_data[\"train\"].column_names,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
       "    num_rows: 410964\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = noanswer_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_fn, is_batched = PREP_PIPELINE[\"extractive\"](\n",
    "    tokenizer, \"eval\", data_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = noanswer_data[\"validation\"].map(\n",
    "    prep_fn,\n",
    "    batched=is_batched,\n",
    "    num_proc=data_args.preprocessing_num_workers,\n",
    "    remove_columns=korquad_data[\"validation\"].column_names,\n",
    "    load_from_cache_file=not data_args.overwrite_cache,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datasets = noanswer_data[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.reader import READER_HOST\n",
    "from solution.data.metrics import compute_metrics\n",
    "\n",
    "from solution.reader.architectures import MODEL_INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_cls = READER_HOST[\"extractive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args.model_name_or_path = \"./outputs/koelectra_v3_test2\"\n",
    "# model_args.architectures = \"RobertaForQAWithConvSDSHead\"\n",
    "# model_args.model_init = \"qaconv_head\"\n",
    "# model_args.use_auth_token = True\n",
    "# model_args.reader_type = \"extractive\"\n",
    "# model_args.model_head = \"sds_conv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution.data.processors.post import post_processing_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"korquadv1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.run_name = \"korquad\"\n",
    "training_args.output_dir = \"outputs/korquad\"\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.per_device_train_batch_size = 12\n",
    "training_args.per_device_eval_batch_size = 12\n",
    "training_args.gradient_accumulation_steps = 1\n",
    "training_args.learning_rate = 5e-5\n",
    "training_args.fp16 = True\n",
    "training_args.max_answer_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = reader_cls(model_args, tokenizer)\n",
    "reader.set_trainer(\n",
    "    model_init=reader.model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_features,\n",
    "    eval_dataset=valid_features,\n",
    "    eval_examples=valid_datasets,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    post_process_function=post_processing_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
    "Model config RobertaConfig {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 1024,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 4096,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"tokenizer_class\": \"BertTokenizer\",\n",
    "  \"transformers_version\": \"4.12.0.dev0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 32000\n",
    "}\n",
    "\n",
    "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
    "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForQAWithConvSDSHead: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
    "- This IS expected if you are initializing RobertaForQAWithConvSDSHead from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing RobertaForQAWithConvSDSHead from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "Some weights of RobertaForQAWithConvSDSHead were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['qa_outputs.convs.1.conv2.weight', 'qa_outputs.convs.4.conv1.weight', 'qa_outputs.convs.1.layer_norm.bias', 'qa_outputs.convs.0.layer_norm.bias', 'qa_outputs.convs.4.conv1.bias', 'qa_outputs.convs.2.layer_norm.weight', 'qa_outputs.convs.1.conv1.weight', 'qa_outputs.convs.0.conv2.weight', 'qa_outputs.convs.2.conv1.weight', 'qa_outputs.convs.1.conv1.bias', 'qa_outputs.convs.0.conv1.bias', 'qa_outputs.convs.0.conv1.weight', 'qa_outputs.convs.0.conv2.bias', 'qa_outputs.convs.2.conv1.bias', 'qa_outputs.qa_output.weight', 'qa_outputs.convs.3.conv1.bias', 'qa_outputs.convs.2.layer_norm.bias', 'qa_outputs.convs.2.conv2.bias', 'qa_outputs.convs.1.layer_norm.weight', 'qa_outputs.convs.3.conv2.bias', 'qa_outputs.convs.0.layer_norm.weight', 'qa_outputs.convs.3.layer_norm.weight', 'qa_outputs.convs.4.conv2.bias', 'qa_outputs.convs.3.layer_norm.bias', 'qa_outputs.convs.4.conv2.weight', 'qa_outputs.qa_output.bias', 'qa_outputs.convs.3.conv1.weight', 'qa_outputs.convs.3.conv2.weight', 'qa_outputs.convs.4.layer_norm.weight', 'qa_outputs.convs.4.layer_norm.bias', 'qa_outputs.convs.2.conv2.weight', 'qa_outputs.convs.1.conv2.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "Using amp fp16 backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with reader.mode_change(mode=\"train\"):\n",
    "    train_results = reader.read(resume_from_checkpoint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch\tTraining Loss\tValidation Loss\tExact Match\tF1\n",
    "1\t0.690800\tNo log\t79.425009\t86.229322\n",
    "2\t0.543700\tNo log\t79.736751\t86.170849\n",
    "3\t0.409500\tNo log\t79.857984\t86.449094\n",
    "4\t0.310300\tNo log\t80.498788\t86.746011\n",
    "5\t0.186200\tNo log\t80.689297\t87.123838\n",
    "6\t0.111100\tNo log\t82.213370\t88.365081\n",
    "7\t0.063800\tNo log\t83.304468\t89.064874\n",
    "8\t0.022400\tNo log\t83.200554\t88.944423\n",
    "9\t0.008600\tNo log\t84.187738\t89.685415\n",
    "10\t0.007800\tNo log\t84.447523\t89.923963\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reader._trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.architectures.append(\"RobertaForQuestionAnswering\")\n",
    "model.config._name_or_path = \"jinmang2/roberta-large-qaconv-sds-korquad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.name_or_path = \"jinmang2/roberta-large-qaconv-sds-korquad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\n",
    "    \"jinmang2/roberta-large-qaconv-sds-korquad\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "    \"jinmang2/roberta-large-qaconv-sds-korquad\",\n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
